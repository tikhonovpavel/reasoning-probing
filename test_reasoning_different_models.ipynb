{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tikhonov/.conda/envs/condaenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"openai/gpt-oss-20b\",\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "# ).to('cuda:1')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tikhonov/.conda/envs/condaenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-09-17\\n\\nReasoning: medium\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\\n\\nYOYOYOYOYUOY\\n\\n<|end|><|start|>user<|message|>341*6432<|end|><|start|>assistant'\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"YOYOYOYOYUOY\"},\n",
        "    {\"role\": \"user\", \"content\": \"341*6432\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "print(repr(tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=False)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Manually constructed prompt string ---\n",
            "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-09-17\\n\\nReasoning: medium\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>341*6432<|end|><|start|>assistant<|channel|>analysis<|message|>We are a language model. The user gives a multiplication problem \"341*6432\". They likely want the product. So compute 341 *'\n",
            "\n",
            "--- Tokenizing and generating ---\n",
            "\n",
            "--- Model Generation (what it added) ---\n",
            " 6432. Let's do it manually: 6432 * 341 = 6432 * 300 + 6432 * 40 + 6432 * 1. 6432*300 = 1,929,600 (since 6432*100=643200, times 3). 6432*40 = 257,280. 6432 = 6,432. Sum: 1,929,600 + 257,280 = 2,186,880. Then add 6,432: Total 2,193,312. Alternatively more precisely: Let's multiply: 6432 * 341. Check: 6432 * 341: 6432 * (300 + 40 + 1) as above. 6432*300=1,929,600. 6432*40=257,280. 6432*1=6,432. Add: 1,929,600+257,280=2,186,880. +6,432=2,193,312. So answer 2,193,312.\n",
            "\n",
            "Thus output: 2,193,312.<|end|><|start|>assistant<|channel|>final<|message|>The product is:\n",
            "\n",
            "\\(341 \\times 6\\,432 = 2{,}193{,}312\\).<|return|>\n"
          ]
        }
      ],
      "source": [
        "# 1. Формируем историю с помощью apply_chat_template\n",
        "messages = [\n",
        "\n",
        "    {\"role\": \"user\", \"content\": \"341*6432\"}\n",
        "]\n",
        "\n",
        "# Получаем строку со всеми системными промптами и историей, но без промпта для генерации\n",
        "prompt_history = tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
        "\n",
        "# 2. Вручную добавляем \"подставные\" мысли, НЕ закрывая теги\n",
        "# Возьмем часть рассуждений из первого рабочего примера\n",
        "qwen_reasoning_part = \"We are a language model. The user gives a multiplication problem \\\"341*6432\\\". They likely want the product. So compute 341 *\"\n",
        "manual_part = f\"<|start|>assistant<|channel|>analysis<|message|>{qwen_reasoning_part}\"\n",
        "\n",
        "full_prompt_string = prompt_history + manual_part\n",
        "\n",
        "print(\"--- Manually constructed prompt string ---\")\n",
        "print(repr(full_prompt_string))\n",
        "print(\"\\n--- Tokenizing and generating ---\")\n",
        "\n",
        "# 3. Токенизируем и генерируем\n",
        "inputs = tokenizer(full_prompt_string, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=2048)\n",
        "\n",
        "# 4. Декодируем только новую, сгенерированную часть\n",
        "generated_part = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=False)\n",
        "\n",
        "print(\"\\n--- Model Generation (what it added) ---\")\n",
        "print(generated_part)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c95468113a44438938b62334968b5b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91aa86eb115f4c2ea8fed419bdc8cb28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "['140', '1']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\", dtype=torch.bfloat16, device_map='auto')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['140', '10']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "tokenizer.tokenize('14010')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<｜begin▁of▁sentence｜>Answer only with the number. Don't write any explanations.<｜User｜>23+17<｜Assistant｜><think>\n",
            "\n",
            "<｜begin▁of▁sentence｜><｜begin▁of▁sentence｜>Answer only with the number. Don't write any explanations.<｜User｜>23+17<｜Assistant｜><think>\n",
            "I need to add the numbers 23 and 17 together.\n",
            "\n",
            "First, I'll add the units digits: 3 plus 7 equals 10.\n",
            "\n",
            "Next, I'll add the tens digits: 2 plus 1 equals 3.\n",
            "\n",
            "Finally, I'll combine these results to get the total sum.\n",
            "</think>\n",
            "\n",
            "23 + 17 equals 40.\n",
            "\n",
            "**Step-by-Step Explanation:**\n",
            "\n",
            "1. **Add the units place:**\n",
            "   \n",
            "   \\[\n",
            "   3 \\ (\\text{from } 23) + 7 \\ (\\text{from } 17) = 10\n",
            "   \\]\n",
            "\n",
            "2. **Add the tens place:**\n",
            "   \n",
            "   \\[\n",
            "   2 \\ (\\text{from } 23) + 1 \\ (\\text{from } 17) = 3\n",
            "   \\]\n",
            "\n",
            "3. **Combine the results:**\n",
            "   \n",
            "   \\[\n",
            "   3 \\ (\\text{tens}) \\times 10 + 0 \\ (\\text{units}) = 40\n",
            "   \\]\n",
            "\n",
            "**Final Answer:** \\boxed{40}<｜end▁of▁sentence｜>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Answer only with the number. Don't write any explanations.\"},\n",
        "    {\"role\": \"user\", \"content\": \"23+17\"}\n",
        "]\n",
        "\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "print(str(prompt))\n",
        "\n",
        "\n",
        "output = model.generate(tokenizer(prompt, return_tensors='pt').input_ids.to(model.device), max_new_tokens=2048)\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<｜begin▁of▁sentence｜><｜User｜>23+17<｜Assistant｜><think>test thinking<think>40<｜end▁of▁sentence｜>'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": \"23+17\"}, {\"role\": \"assistant\", \"content\": \"<think>test thinking</THINK>40\"}],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=False\n",
        ").replace(\n",
        " \n",
        "    '</THINK>', '<think>'\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "condaenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
