# Сетап

* собрал цепочки по GPQA-Diamond (198 вопросов) на Qwen3-32B с помощью OpenRouter API  
  * взял Qwen3-32B, потому что в thinking mode у него заявлен аккураси 68.4% , а какой-нибудь Qwen3-1.7B \- 28.6%, что почти случайный шанс.   
  * в промпте указывал {"role": "system", "content": "Respond only with the letter of the correct option. Don't write any explanations"}. В 95% случаев всё было ок, он генерил нормальный такой ризонинг внутри \<think\> тэга, а после него писал сразу букву ответа. В 5% случаев он и сам \<think\>\</think\> пустым генерил, но в этих случаях я просто ресэмплил, и всё становилось ок.  
* затем собрал хиддены на последнем 64м слое, прогнав собранные цепочки через локальный Qwen3-32B сохраняя их в отдельный файл  
* обучал в 4 вариантах:  
  * первая ось:  
    * либо Wu @ LN(A @ h\_t \+ b)   
    * либо LSTM как в [https://www.arxiv.org/pdf/2506.02536v1](https://www.arxiv.org/pdf/2506.02536v1) (т.е. вся цепочка хидденов с 64го слоя квена до текущего токена. lstm\_hidden\_size=128)   
  * вторая ось:  
    * либо за таргет берём ground\_truth метку  
    * либо за таргет берём model\_answer \- то что квен сам решил является ответом на задачу

	 лосс считаю только на 4 позиции, соответствующие токенам A, B, C и D.

# Результаты (на 31.07.2025):

1. Просто гистограмма длин ризонинг цепочек в количестве токенов.  
   ![][image1]

2. График аккураси (таргет – model\_answer) на валидации по эпохам, который я сразу разделил по квантилям в зависимости от того в какой части ризонинг цепочки стоит данный токен \- ближе к началу, ближе к середине, ближе к концу ли:

|  L S TM | Предсказываем model\_answer  ![][image2]  | Предсказываем ground\_truth  ![][image3] |
| :---: | ----- | :---: |
|  L I N E A R | ![][image4] | ![][image5] |

У LSTM качество повыше будет, и графики более выраженные. Это на самом деле очень любопытное:

Для model\_answer видно в общем, что качество предсказания увеличивается, чем дальше к концу рассуждения. Причём довольно равномерно, если судить по среднему.  
А вот для ground\_truth для квантилей 60-80% и 80-100% – раньше остальных происходит переобучение и их качество ползёт вниз, а **первые квантили вообще вырвались вперёд и обогнали даже аккураси по последним,** даже когда те ещё не переобучились.

**Значит ли это, что мы нашли то, что модель в процессе ризонинга сама себя запутывает, и и приходит к неверным ответам, хотя изначально была к этому ответу ближе?**

3. Этот же график для LSTM model\_answer, для 64й эпохи, но более детальный – не с 5 группами, а с 20 (теперь по иксу не эпохи, а позиция в ризонинг трейсе):

![][image6]

	Я хотел увидеть более интересный паттерн, чем монотонное возрастание, но по-моему ничего такого нет.

4. Тогда я сделал ещё потокенную визуализаци.. На таком уровне гранулярности тоже каких-то паттернов мне не удалось обнаружить.

Но видно, что предсказания ооочень шумные. То есть **я ни разу не увидел, что на гистограмме какая-то метка имеет вероятность выше 40%. Распределение почти всегда похоже на равномерное.**

Слева – синим выделены те токены, аргмакс по предсказаниям LSTM для которых дал верный таргет.  
Справа сверху – график распределений предсказаний по текущему токену. Справа снизу – статичный график, сколько аргмаксов по предсказанию LSTM по окну из последних 10 токенов совпали с таргетом.

![][image7]

![][image8]

Приглашаю потыкать)) [https://tikhonovpavel.github.io/reasoning-probe-classifier-visualizer/](https://tikhonovpavel.github.io/reasoning-probe-classifier-visualizer/) 

P.S. Есть подозрение, что данных маловато, поэтому качество довольно низкое. Поэтому надо взять не diamond, а например main \- там в 2 раза больше (448 вопросов vs 198).

---

# 01.08.2025

Попробовал разные формулировки инпут промпта. Один прогон датасета на таких коротких промптах прогоняется очень быстро – примерно за минуту. Поэтому почти везде считал на 10 запусках, чтобы оценить отклонение.

## Без no-think

Здесь я просто добавляю \\boxed{ в начало ответа модели.  
Внутри \<think\> и \</think\> просто два переноса строки, как в стандартном режиме квена при параметре enable\_thinking=False.

![][image9]

Total runs: 10  
Mean Accuracy: 44.55% ± 1.29%

![][image10]

---

Здесь внутри \<think\> и \</think\>: Okay, I have finished thinking..

![][image11]

Total runs: 10  
Mean Accuracy: 0.4323 ± 0.0261  
Mean Accuracy: 43.23% ± 2.61%

![][image12]

---

## No-think

Попробовал с no-think промптом:

![][image13]  
Total runs: 10  
Mean Accuracy: 43.24% ± 1.72%

![][image14]

---

Здесь снова с фразой Okay, I have finished thinking..

![][image15]

Total runs: 10  
Mean Accuracy: 42.01% ± 2.18%

![][image16]

---

## Разный контент внутри \<think\>...\</think\>

 Гипотеза такая: возможно модель правда думает в латентах и семантика ризонинга не так важна, .

Используем первое предложение из реального трейса, сгенеренного моделью. Оно почти всегда какое-то общее и более-менее одинаковое для всех промптов (ниже несколько примеров), но возможна раз они примерно одинаковые, в них она и засовывает реальное размышление в латенты:

Okay, let's see.  
Okay, let's try to figure this out.  
Okay, let me try to work through this problem step by step.  
Okay, let's tackle this problem step by step.  
Okay, let's try to figure this out step by step.  
Okay, let's try to figure out this problem.

Total runs: 10  
Mean Accuracy: 43.89% ± 2.30%

![][image17]

---

В тех репорте Qwen3 у них есть фраза:  
![][image18]

Здесь я снова использую только первое предложение из реального трейса, сгенеренного моделью, но добавляю их специальную фразу. Если она с ней обучалась, то может быть для неё это триггер – может она выучила, что ахтунг, и больше ей не дадут подумать, и надо скорее соображать.

reasoning\_trace \= f"\\n{reasoning\_trace}.\\nConsidering the limited time by the user, I have to give the solution based on the thinking directly now.\\n"

Total runs: 10  
Mean Accuracy: 43.64% ± 1.45%

![][image19]

---

И ещё один эксперимент, снова добавляю фразу  “Considering the limited time by the user, I have to give the solution based on the thinking directly now.” но использую не одно, а 10 предложений из настоящего трейса (учитывая что порой длина доходит до 600–1000 предложений, это немного).

![][image20]

Total runs: 10  
Mean Accuracy: 42.53% ± 0.98%

![][image21]

---

Sanity check – проверим не на 10 предложениях, а на 200\.

Total runs: 5  
Mean Accuracy: 60.71% ± 1.31%

Напомню что на полных трейсах качество было 65.7%

---

| *05.08.2025  Если мы делаем early stopping как сейчас и модель выдаёт неправильный ответ, это ещё не значит, что в латентах его нет (может она просто не привыкла к такому режиму, что её посередине прерывают и поэтому выдаёт что-то рандомное, не смотря на то, что в хидденах уже всё было правильно). Поэтому пробинг классификатор всё равно нужен. Но в данном случае, если мы используем лстм (по собранным ризонинг цепочкам), аккураси у лстм хуже, чем у самой модели, что странно. Это может быть вследствие того, что лстм просто забывает чему соответствуют метки A, B, C, D \- каким именно ответам. А сама ллм может думать про ответ не в терминах A, B, C, D, а в терминах самого числового ответа. Поэтому в качестве probing classifier логичнее обучать однослойный аттеншен вместо лстм. Плюс по нему можно будет посмотреть мапу внимания, а не просто на метрику качества. И таргет может быть одним из трёх вариантов: то, что модель предсказала в конце цепочки. ground-truth метка то что модель предскажет, если сейчас остановить ризонинг цепочку. 	И в каждом из случаев мапа внимания даст нам немного разную информацию.*  |
| :---- |

---

**10.08.2025**

по идее c non-thinking провёл экспы по \`\<think\>Okay, I have finished thinking.\</think\>\`. На общей встрече во вторник, обсудили ещё несколько идей развития. Из них я попробовал уже одну с аттеншеном: написал новый классификатор, провёл с ним эксп по собранным цепочкам. Но оказалось что он очень сильно переобучается: к 3й эпохе аккураси на трейне 80%, а на тесте 30% и val-loss загибается. Я добавил слой-проекцию до слоя аттеншена (у второго \- квадратичное кол-во весов) \+ добавил агрессивный weight decay \- не помогло. Скорее всего просто маленький датасет с реальными ризонинг цепочками. Как идея \- взять какой-нибудь [https://huggingface.co/datasets/open-r1/OpenThoughts-114k-math](https://huggingface.co/datasets/open-r1/OpenThoughts-114k-math) в котором 90к таких трейсов. 

**20.08.2025**

Я понял почему аттеншен переобучался так сильно) Он мог просто выучить такие ключи/значения, что скоры для всех токенов кроме какого-нибудь токена из вопроса были бы нулевыми. Он научился выделять вопрос и стало неважно для какого токена из последовательности мы предсказываем, и он мог просто выучить, что этому вопросу соответствует такой-то ответ и всё.  
Поэтому в дальнейшем я буду использовать только те пробинги, которые используют только текущий хидден.  
---

**03.09.2025**

Я стал обучать пробинг модели на хиддены токенов промпта “Okay, I have finished thinking”. Я обучал отдельный двухслойный MLP для каждого токена.  
Но долгое время это не приводило ни к каким результатам: 

1) Для GPQA предсказания были на уровне случайного угадывания (максимум у меня был 28% accuracy).   
2) Я подумал, что GPQA просто слишком маленький датасет и взял [https://huggingface.co/datasets/PrimeIntellect/NuminaMath-QwQ-CoT-5M](https://huggingface.co/datasets/PrimeIntellect/NuminaMath-QwQ-CoT-5M), оставил только те задачи, в которых ответ является числом, и **обучал на задачу регрессии, вместо классификации** на сабсете из 50к таких задач (+ такой подход должен был бы избавить от проблемы, что в хиддене может не храниться инфы о том, какому ответу соответствует та или иная буква варианта ответа). Но и здесь результаты выходили не очень. 

	Я менял гиперпараметры, пробовал разные архитектуры, много чего ещё колдовал, но качество всё равно было на уровне случайного угадывания. Наконец я решил **напрямую посмотреть, а насколько различаются хиддены на токенах нашего фиксированного промпта “Okay, I have finished thinking”, когда он используется для двух разных вопросов.**

Для этого я просэмплировал из GPQA по 100 пар вопросов на каждую из вариаций эксперимента:

* ответ на оба вопроса – одна и та же буква (например обе B)  
* ответ на каждый из вопросов – различные буквы (например C и A)

	Далее я считал cosine similarity между хидденами **одноимённых токенов** нашего фиксированного промпта из каждой пары. То есть, я брал хидден токена "Okay" из промпта для первого вопроса и хидден токена "Okay" из промпта для второго вопроса и сравнивал их. Эту операцию я проделывал для каждого токена в “Okay, I have finished thinking”, а затем усреднял результат по всем 100 парам.

| ![][image22] | ![][image23] |
| :---- | :---- |

Первое, что меня удивительно, естественно, насколько оно всё близко к единице. Для Qwen3-32B (слева) значение cosine similarity опускается ниже 0.9 практически только на самом первом токене “Okay”. Всё остальное время оно \>0.9 и часто даже \>0.95.  
	Для QwQ-32B (справа) чуть отличается паттерн, но тоже очень большая similarity \>0.95 почти всегда.

В качестве sanity check я построил такой же график, но для первых 15 токенов вопроса в каждой паре:  
![][image24]

Понятно, что в отличие от предыдущих вопросов здесь сами токены различны, поэтому similarity должна быть ещё ниже (не уверен почему такая большая схожесть на самом первом токене. Возможно потому что это только часть целого слова, и модель ещё совсем не понимает о чём пойдёт речь).

Ещё мне показалось интересным, что на позапрошлой картинке токен Okay отличался от других, поэтому я взял фразу “Okay, let’s see”, как говорил ранее здесь: [Reasoning Probing](https://docs.google.com/document/d/1IrRvDQQTyezNC2CLzvWhXN6DClJz6eWTVpbp7_mgXSE/edit?tab=t.0#bookmark=id.400j7y1db6gw) – значительная часть всех размышлений модели начинаются с этой фразы, и возможно она научилась внутри них параллельно и думать тоже:

![][image25]

Как будто действительно, с этой фразой, состояния при разных вопросах разнятся больше, чем при нашей фразе “Okay, I have finished thinking”.

**05.09.2025**

Вот распределение значений ответов в датасете с числовым ответом:  
**![][image26]**

Я его логарифмировал и обучал уже на такой:

**![][image27]**

Получил такую RMSE метрику на валидацию с использованием Attention-probing модели при обучении на датасете из 10к примеров (layer 40 / 64). 

| ![][image28] | \- Token 'Okay' (0) \- Token ',' (1) \- Token 'ĠI' (2) \- Token 'Ġhave' (3) \- Token 'Ġfinished' (4) \- Token 'Ġthinking' (5) \- Token '.' (6) \- Token 'ĠThe' (7) \- Token 'Ġfinal' (8) \- Token 'Ġanswer' (9) \- Token 'Ġis' (10) |
| :---- | ----: |

Ошибка почти не падает и остаётся довольно большой учитывая значения самих .

**11.09.2025**

**Я подумал, что мы можем смотреть не только на финальный ответ, но и на генерацию самого ризонинга.**

Мне в общем пришла идея: разделить промпт на чанки по токенам "Wait", "Alternatively" и т.д.   
Если от того, что мы выкидываем какие-то чанки, перплексия по остальным сильно не меняется, то значит ей не сильно то и нужен был реально сгенеренный текст.

Я запилил интерфейс:

![][image29]

и проверил на 20 различных примерах, что будет происходить с перплексией на уровне целых чанков и на уровне токенов. По результатам: 

* с одной стороны, можно, конечно сказать, что перплексия увеличивается, но совершенно не так, как ожидалось. Оказывается, если выкинуть чанки в которых происходили некоторые вычисления, то в последующих чанках, когда по контексту подразумевается использование вычисленных значений, то если дать модели генерить с этого момента, она не пишет сразу результат, а реально проводит явное вычисление. 

На самом деле хорошее свидетельство в пользу того, что модель всё таки вычисляет с помощью токенов, а не посчитала всё в хидденах с самого начала, а дальше сидит и генерит текст псевдоразмышления для вида. Иначе зачем иметь структурированный 

Есть несколько идей:

* заменить числа на правдоподобные, но ложные. Пересчитать PPL и посмотреть, «копирует» ли модель ложь или пере­вычисляет.

* Стабильность к парафразам и шуму

![][image30]

![][image31]

---

**17.09.2025**

Я разделил реальные ризонинг трейсы по токенам Wait, Hmm, Alternatively и т.д. на чанки. Затем, после каждого чанка ставил \</think\> и модель выдавала её лучшее предположение об ответе на уровне этого чанка. С какого-то момента, модель начинала правильно выдавать только один и тот же ответ, и я назвал первый такой чанк \- "критический чанк"

![][image32]  
Этот график показывает, как точность модели зависит от длины её цепочки рассуждений. Каждый столбик — группа задач, сгруппированных по количеству чанков в рассуждении модели. Общая высота каждого столбика — это итоговая точность модели для задач данной длины. Число над столбиком дублирует эту точность, а под ним в скобках указано общее количество примеров в этом бине (n=...) и какой процент они составляют от всего набора данных.  
Сам столбик разделён на цветные сегменты, которые показывают, как часто модель меняла своё мнение в процессе решения. Легенда справа поясняет, какой цвет соответствует какому количеству переключений ответа (0, 1, 2, 3+). Числа внутри каждого сегмента дают более детальную информацию: верхнее число — это точность для данной конкретной подгруппы (например, точность только для тех случаев, где не было переключений), а нижнее 'n=...' — это количество таких примеров в бине.

Тут тренд такой, что, чем более длинное решение модель генерирует (вполне вероятно, что это говорит, что эта задача более сложная), тем в таких цепочках модель реже ответ появляется сразу на первом же чанке. Модель начинает менять решение иногда один раз, иногда 2, а иногда даже больше раз.

Можно посмотреть на это немного под другим углом и задаться вопросом, насколько часто модель отвечает правильно, если за время генерации ризонинга никогда не меняет решения? Или меняет 1 раз, или 2, и т.д.?  
Получаем что-то такое:

| Number of Changes | Accuracy |
| :---: | :---: |
| 0 | \~65% |
| 1 | \~70% |
| \>1 | \<50% |

Т.е. если модель сменила ответ единожды, то её аккураси составляет \~70%. Напрашивается вывод \- если мы придумаем эвристику, и станем ловить момент, когда модель поменяла решение окончательно, то мы можем очень сильно сократить цепочку и оставить приемлемое качество.  
	

Затем я посмотрел, как именно меняется вероятность от чанка к чанку. Сначала я думал, что модель приходит к правильному ответу постепенно, тогда просто от чанка к чанку, вероятность правильного ответа должна просто потихоньку расти.   
	Что происходит с уверенностью модели в момент когда она меняет решение на другое?

| ![][image33] | ![][image34] |
| :---: | :---: |

Но по факту оказалось, что если сравнивать вероятность предсказать правильный ответ на критическом чанке k с вероятностью предсказать правильный ответ на чанке (k-1), она меняется ОЧЕНЬ сильно.

Это кажется довольно многообещающим. Вероятность определённого ответа действительно **резко** подскакивает, часто больше чем на 50%, а то и 75% от того, какой была на предыдущем чанке и в большинстве случаев при такой резкой смене вероятности, этот новый ответ оказывается правильным.   
Интересно, что иногда модель становится уверена в обратном, точнее в **неверном** ответе. И это тоже выражается резким изменением вероятности, хоть и не столь драматичном, характерном для правильного ответа.

То есть уверенность модели в ответе подскакивает не равномерно, а наоборот. Кажется это свидетельство в пользу гипотезы о том, что важна именно семантика, а не хиддены.

	Это графики того, где по отношению к длине всего ризонинга находится критический чанк (здесь мы берём только те случаи, когда произошёл только один свитч ответа. То есть сначала модель предсказывала X, затем с какого-то момента только Y):

![][image35]

Это то, сколько чанков мы можем сэкономить, если будем делать early stopping после резкого изменения вероятностей:  
![][image36]

Чтобы дополнительно убедиться в том, что дело именно в семантике текста генерируемого ризонинга, а не в состояниях хидденов, я стал засовывать ризонинг, полученный с модели Qwen3-32B, на котором проводились все предыдущие эксперименты в модель gpt-oss-20b. **Сможет ли gpt-oss-20b самостоятельно прийти к тому же выводу, что и Qwen, оттолкнувшись от её мыслей**?

В нескольких сэтапах проводил эксп.:

1) На чанке k: Подавал в gpt-oss-20b срез рассуждений Qwen до k-го чанка включительно и сразу извлекал предсказание.  
2) На чанках k-1 и k-2: Аналогично, но использовал срезы рассуждений до k-1-го и k-2-го чанков соответственно.  
3) Продолжение с чанков k-1 и k-2: Подавал в gpt-oss-20b срезы рассуждений до k-1-го (и k-2-го), позволял модели сгенерировать 128 токенов собственного продолжения, и только после этого извлекал её предсказание.

![][image37]

\- получается, что для критического чанка имеем самую большую аккураси. А также, продолжать с (k-1)-го заметно эффективнее, чем с (k-2)-го чанка.

я бы всё таки сказал, что это свидетельство в пользу того, что модель всё-таки думает с помощью текста, а не просто в весах или латентах

---

то есть, если это публиковать, можно два вывода сделать: 

* во первых предложить способ делать early stopping, не обучая никаких пробингов для разных архитектур, а просто судить по изменению уверенности модели в каком-то из ответов, периодически опрашивая её во время фазы ризонинга.  
* а второе, показать, что ризонинг переносится между моделями

---

**18.09.2025**

	Ранее мы рассматривали упрощённый случай – мы рассматривали только те случаи, когда произошёл только один свитч ответа (см. график “Stacked Accuracy vs Reasoning Length” чтобы оценить вклад этого случая). Мы показали, что в такой ситуации действительно существует скачок между чанком k-1 и чанком k, где чанк k \- это такой чанк, после которого ответ больше не меняется.   
То есть мы поняли, что такой феномен действительно есть, но чтобы понять, насколько хорошо будет работать эвристика на его основе, нужно провести новый анализ.

Будем применять следующее правило: смотрим, как меняется вероятность каждого из ответов (A, B, C, D) от одного чанка к другому. Если вероятность какой-либо буквы резко возрастает — то есть разница между её вероятностью на текущем шаге и на предыдущем шаге превышает заданный порог (threshold) — делаем в этот момент early stopping.

Формально это можно записать так: триггер срабатывает, если для любой из букв выполняется условие P(буква)\_k \- P(буква)\_(k-1) \> threshold.  
P(буква)\_k — это вероятность буквы на текущем шаге k.  
P(буква)\_(k-1) — это вероятность той же буквы на предыдущем шаге k-1.

(Если на одном шаге порог превышают сразу несколько букв, то в качестве ответа выбирается та, у которой итоговая вероятность (P(буква)\_k) оказалась самой высокой)

	Вот результаты:

![][image38]  
**Max Overall Accuracy: 72.38% achieved at Threshold=0.24**  
  \- Coverage: 55.95% (235/420)  
  \- Precision: 80.00% (188/235)  
 \- Avg Chunk Savings (per trigger): 12.69  
  \- Overall Avg Chunk Savings: 7.10   
\- Avg Token Savings (per trigger): 44.80  
  \- Overall Avg Token Savings: 25.07

А вот это – очень неожиданно. Если мы применяем эвристику выхода при росте уверенности в ответе между чанками — то аккураси в принципе по датасету **увеличивается** (\!). У нас не то что трейдофф какой-то надо искать, между тем сколько токенов сэкономим и сколько в аккураси потеряем. Мы наоборот – получаем прирост по аккураси, если выходим как только уверенность в каком-то токене резко выросла)) модель сама себя сбивает и приходит к неправильному ответу?

Этот вывод точно нужно использовать в статье)
