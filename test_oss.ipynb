{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tikhonov/.conda/envs/condaenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"openai/gpt-oss-20b\",\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "# ).to('cuda:1')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-09-17\\n\\nReasoning: medium\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\\n\\nYOYOYOYOYUOY\\n\\n<|end|><|start|>user<|message|>341*6432<|end|><|start|>assistant'\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"YOYOYOYOYUOY\"},\n",
        "    {\"role\": \"user\", \"content\": \"341*6432\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "print(repr(tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=False)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Manually constructed prompt string ---\n",
            "'<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-09-17\\n\\nReasoning: medium\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>341*6432<|end|><|start|>assistant<|channel|>analysis<|message|>We are a language model. The user gives a multiplication problem \"341*6432\". They likely want the product. So compute 341 *'\n",
            "\n",
            "--- Tokenizing and generating ---\n",
            "\n",
            "--- Model Generation (what it added) ---\n",
            " 6432. Let's do it manually: 6432 * 341 = 6432 * 300 + 6432 * 40 + 6432 * 1. 6432*300 = 1,929,600 (since 6432*100=643200, times 3). 6432*40 = 257,280. 6432 = 6,432. Sum: 1,929,600 + 257,280 = 2,186,880. Then add 6,432: Total 2,193,312. Alternatively more precisely: Let's multiply: 6432 * 341. Check: 6432 * 341: 6432 * (300 + 40 + 1) as above. 6432*300=1,929,600. 6432*40=257,280. 6432*1=6,432. Add: 1,929,600+257,280=2,186,880. +6,432=2,193,312. So answer 2,193,312.\n",
            "\n",
            "Thus output: 2,193,312.<|end|><|start|>assistant<|channel|>final<|message|>The product is:\n",
            "\n",
            "\\(341 \\times 6\\,432 = 2{,}193{,}312\\).<|return|>\n"
          ]
        }
      ],
      "source": [
        "# 1. Формируем историю с помощью apply_chat_template\n",
        "messages = [\n",
        "\n",
        "    {\"role\": \"user\", \"content\": \"341*6432\"}\n",
        "]\n",
        "\n",
        "# Получаем строку со всеми системными промптами и историей, но без промпта для генерации\n",
        "prompt_history = tokenizer.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)\n",
        "\n",
        "# 2. Вручную добавляем \"подставные\" мысли, НЕ закрывая теги\n",
        "# Возьмем часть рассуждений из первого рабочего примера\n",
        "qwen_reasoning_part = \"We are a language model. The user gives a multiplication problem \\\"341*6432\\\". They likely want the product. So compute 341 *\"\n",
        "manual_part = f\"<|start|>assistant<|channel|>analysis<|message|>{qwen_reasoning_part}\"\n",
        "\n",
        "full_prompt_string = prompt_history + manual_part\n",
        "\n",
        "print(\"--- Manually constructed prompt string ---\")\n",
        "print(repr(full_prompt_string))\n",
        "print(\"\\n--- Tokenizing and generating ---\")\n",
        "\n",
        "# 3. Токенизируем и генерируем\n",
        "inputs = tokenizer(full_prompt_string, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=2048)\n",
        "\n",
        "# 4. Декодируем только новую, сгенерированную часть\n",
        "generated_part = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=False)\n",
        "\n",
        "print(\"\\n--- Model Generation (what it added) ---\")\n",
        "print(generated_part)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "condaenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
